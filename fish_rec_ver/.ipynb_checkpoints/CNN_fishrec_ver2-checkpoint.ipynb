{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 資料準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-81efa928a5f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;31m#import dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'data'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import data\n",
    "#import dataset\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from datetime import timedelta\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 設定參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of color channels for the images: 1 channel for gray-scale.\n",
    "num_channels =3\n",
    "\n",
    "# image dimensions (only squares for now)\n",
    "img_size = 128\n",
    "\n",
    "# Size of image when flattened to a single dimension\n",
    "img_size_flat = img_size * img_size * num_channels\n",
    "\n",
    "# Tuple with height and width of images used to reshape arrays.\n",
    "img_shape = (img_size, img_size)\n",
    "\n",
    "# class info\n",
    "classes = ['Brook','Clown','Gold','Soldier','White']\n",
    "#classes = ['0','1','2','3','4','5','6','7','8','9']\n",
    "num_classes = len(classes)\n",
    "\n",
    "# batch size\n",
    "batchSize = 100\n",
    "\n",
    "# validation split\n",
    "validation_size = .15\n",
    "testdation_size = .15\n",
    "\n",
    "# how long to wait after validation loss stops improving before terminating training\n",
    "early_stopping = None  # use None if you don't want to implement early stoping\n",
    "\n",
    "train_path = 'U:/git/fish_Recognition/example/train'\n",
    "test_path = 'U:/git/fish_Recognition/example/test'\n",
    "checkpoint_dir = \"./set_models/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建立共用函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#data = dataset.read_train_sets(train_path, img_size, classes, validation_size=validation_size)\n",
    "#test_data = dataset.read_test_set(test_path, img_size, classes, validation_size=0)\n",
    "data = data.read_train_sets(train_path, img_size, classes, validation_size=validation_size,testdation_size=testdation_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of:\")\n",
    "print(\"- Training-set:\\t\\t{}\".format(len(data.train.labels)))\n",
    "#print(\"- Test-set:\\t\\t{}\".format(len(test_images)))\n",
    "print(\"- Validation-set:\\t{}\".format(len(data.valid.labels)))\n",
    "print(\"-Testdation-set:\\t{}\".format(len(data.test.labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.1),\n",
    "                       name ='W')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias(shape):\n",
    "    return tf.Variable(tf.constant(0.1, shape=shape)\n",
    "                       , name = 'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1,1,1,1], \n",
    "                        padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1], \n",
    "                          strides=[1,2,2,1], \n",
    "                          padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 輸入層 Input Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('Input_Layer'):\n",
    "    x = tf.placeholder(\"float\",shape=[None, img_size_flat]\n",
    "                       ,name=\"x\")    \n",
    "    x_image = tf.reshape(x, [-1, img_size, img_size,num_channels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('C1_Conv'):\n",
    "    W1 = weight([5,5,num_channels,16])\n",
    "    b1 = bias([16])\n",
    "    Conv1=conv2d(x_image, W1)+ b1\n",
    "    C1_Conv = tf.nn.relu(Conv1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('C1_Pool'):\n",
    "    C1_Pool = max_pool_2x2(C1_Conv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('C2_Conv'):\n",
    "    W2 = weight([5,5,16,36])\n",
    "    b2 = bias([36])\n",
    "    Conv2=conv2d(C1_Pool, W2)+ b2\n",
    "    C2_Conv = tf.nn.relu(Conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('C2_Pool'):\n",
    "    C2_Pool = max_pool_2x2(C2_Conv) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('D_Flat'):\n",
    "    layer_shape = C2_Pool.get_shape()\n",
    "    num_features = layer_shape[1:4].num_elements()\n",
    "    D_Flat = tf.reshape(C2_Pool, [-1, num_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('D_Hidden_Layer'):\n",
    "    W3= weight([num_features, 128])\n",
    "    b3= bias([128])\n",
    "    D_Hidden = tf.nn.relu(\n",
    "                  tf.matmul(D_Flat, W3)+b3)\n",
    "    D_Hidden_Dropout= tf.nn.dropout(D_Hidden, \n",
    "                                keep_prob=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 輸出層Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('Output_Layer'):\n",
    "    W4 = weight([128,num_classes])\n",
    "    b4 = bias([num_classes])\n",
    "    y_predict= tf.nn.softmax(\n",
    "                 tf.matmul(D_Hidden_Dropout,\n",
    "                           W4)+b4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 設定訓練模型最佳化步驟"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"optimizer\"):\n",
    "    \n",
    "    y_label = tf.placeholder(\"float\", shape=[None, num_classes], \n",
    "                              name=\"y_label\")\n",
    "    \n",
    "    loss_function = tf.reduce_mean(\n",
    "                      tf.nn.softmax_cross_entropy_with_logits\n",
    "                         (logits=y_predict , \n",
    "                          labels=y_label))\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.00001) \\\n",
    "                    .minimize(loss_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 設定評估模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"evaluate_model\"):\n",
    "    correct_prediction = tf.equal(tf.argmax(y_predict, 1),\n",
    "                                  tf.argmax(y_label, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\")) # 非零為一，的平均差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 訓練模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainEpochs = 200\n",
    "train_batch_size = batchSize\n",
    "valid_batch_size =  428\n",
    "totalBatchs = int(len(data.train.labels)/batchSize)\n",
    "epoch_list=[];accuracy_list=[];loss_list=[];\n",
    "from time import time\n",
    "startTime=time()\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(trainEpochs):\n",
    "\n",
    "    \n",
    "    for i in range(totalBatchs):\n",
    "        #batch_x, batch_y = mnist.train.next_batch(batchSize)\n",
    "        x_batch, y_true_batch, _, cls_batch = data.train.next_batch(train_batch_size)\n",
    "        \n",
    "        # Convert shape from [num examples, rows, columns, depth]\n",
    "        # to [num examples, flattened image shape]\n",
    "        x_batch = x_batch.reshape(train_batch_size, img_size_flat)\n",
    "        \n",
    "        #sess.run(optimizer,feed_dict={x: batch_x, y_label: batch_y})\n",
    "        sess.run(optimizer,feed_dict={x: x_batch, y_label: y_true_batch})\n",
    "    x_valid_batch, y_vtrue_batch, _, valid_cls_batch = data.valid.next_batch(valid_batch_size)\n",
    "    x_valid_batch = x_valid_batch.reshape(valid_batch_size, img_size_flat)\n",
    "    loss,acc = sess.run([loss_function,accuracy],\n",
    "                        feed_dict={x: x_valid_batch, \n",
    "                                   y_label: y_vtrue_batch})\n",
    "\n",
    "    epoch_list.append(epoch)\n",
    "    loss_list.append(loss);accuracy_list.append(acc)    \n",
    "    \n",
    "    print(\"Train Epoch:\", '%02d' % (epoch+1), \\\n",
    "          \"Loss=\",\"{:.9f}\".format(loss),\" Accuracy=\",acc)\n",
    "    \n",
    "duration =time()-startTime\n",
    "print(\"Train Finished takes:\",duration)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(4,2)\n",
    "plt.plot(epoch_list, loss_list, label = 'loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['loss'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epoch_list, accuracy_list,label=\"accuracy\" )\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(4,2)\n",
    "plt.ylim(0.3,1)\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 評估模型準確率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data.test.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 428\n",
    "x_test_batch, y_test_batch, _, test_cls_batch = data.test.next_batch(test_size)\n",
    "x_test_batch = x_test_batch.reshape(test_size, img_size_flat)\n",
    "print(\"Accuracy:\", \n",
    "      sess.run(accuracy,feed_dict={x: x_test_batch,\n",
    "                                   y_label: y_test_batch}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", \n",
    "      sess.run(accuracy,feed_dict={x: x_test_batch[:200],\n",
    "                                   y_label: y_test_batch[:200]}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", \n",
    "      sess.run(accuracy,feed_dict={x:x_test_batch[200:],\n",
    "                                   y_label: y_test_batch[200:]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 預測機率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Fetch argument array([[1.2424151e-10, 7.4186384e-05, 9.9992585e-01, 4.9180056e-08,\n        4.6070851e-12],\n       [1.2405844e-04, 2.4688643e-05, 9.3271933e-07, 1.2063558e-07,\n        9.9985015e-01],\n       [1.9286692e-02, 9.6026134e-01, 1.7305428e-02, 1.8467846e-03,\n        1.2996926e-03],\n       [9.4110735e-02, 2.6780635e-01, 1.6595725e-02, 6.1970866e-01,\n        1.7785546e-03],\n       [2.2106965e-08, 1.9486179e-06, 6.0857014e-06, 1.1412323e-05,\n        9.9998057e-01],\n       [7.9540172e-13, 2.9777493e-05, 9.9984264e-01, 1.2757885e-04,\n        5.4037144e-16],\n       [6.8602812e-01, 1.9168968e-01, 6.6282940e-03, 8.3260022e-02,\n        3.2393940e-02],\n       [7.8717911e-01, 8.3123576e-03, 7.1034902e-03, 1.9738004e-01,\n        2.4930268e-05],\n       [2.5488302e-01, 7.3050028e-01, 8.6609060e-03, 3.3862768e-03,\n        2.5695467e-03],\n       [4.0107757e-02, 4.5974612e-02, 3.6110908e-08, 3.2376161e-01,\n        5.9015602e-01]], dtype=float32) has invalid type <class 'numpy.ndarray'>, must be a string or Tensor. (Can not convert a ndarray into a Tensor or Operation.)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensfw_gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[0;32m    281\u001b[0m         self._unique_fetches.append(ops.get_default_graph().as_graph_element(\n\u001b[1;32m--> 282\u001b[1;33m             fetch, allow_tensor=True, allow_operation=True))\n\u001b[0m\u001b[0;32m    283\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensfw_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[1;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[0;32m   3589\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3590\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3591\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensfw_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[1;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[0;32m   3678\u001b[0m       raise TypeError(\"Can not convert a %s into a %s.\" % (type(obj).__name__,\n\u001b[1;32m-> 3679\u001b[1;33m                                                            types_str))\n\u001b[0m\u001b[0;32m   3680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Can not convert a ndarray into a Tensor or Operation.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-4f96b7f9c865>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m y_predict=sess.run(y_predict, \n\u001b[1;32m----> 2\u001b[1;33m                    feed_dict={x: x_test_batch[:10]})\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensfw_gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensfw_gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m     fetch_handler = _FetchHandler(\n\u001b[1;32m-> 1120\u001b[1;33m         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\n\u001b[0m\u001b[0;32m   1121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m     \u001b[1;31m# Run request and get response.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensfw_gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[0;32m    425\u001b[0m     \"\"\"\n\u001b[0;32m    426\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 427\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_FetchMapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    428\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_targets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensfw_gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[1;34m(fetch)\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m           \u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfetch_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 253\u001b[1;33m           \u001b[1;32mreturn\u001b[0m \u001b[0m_ElementFetchMapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    254\u001b[0m     \u001b[1;31m# Did not find anything.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m     raise TypeError('Fetch argument %r has invalid type %r' % (fetch,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensfw_gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[0;32m    284\u001b[0m         raise TypeError('Fetch argument %r has invalid type %r, '\n\u001b[0;32m    285\u001b[0m                         \u001b[1;34m'must be a string or Tensor. (%s)'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 286\u001b[1;33m                         (fetch, type(fetch), str(e)))\n\u001b[0m\u001b[0;32m    287\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m         raise ValueError('Fetch argument %r cannot be interpreted as a '\n",
      "\u001b[1;31mTypeError\u001b[0m: Fetch argument array([[1.2424151e-10, 7.4186384e-05, 9.9992585e-01, 4.9180056e-08,\n        4.6070851e-12],\n       [1.2405844e-04, 2.4688643e-05, 9.3271933e-07, 1.2063558e-07,\n        9.9985015e-01],\n       [1.9286692e-02, 9.6026134e-01, 1.7305428e-02, 1.8467846e-03,\n        1.2996926e-03],\n       [9.4110735e-02, 2.6780635e-01, 1.6595725e-02, 6.1970866e-01,\n        1.7785546e-03],\n       [2.2106965e-08, 1.9486179e-06, 6.0857014e-06, 1.1412323e-05,\n        9.9998057e-01],\n       [7.9540172e-13, 2.9777493e-05, 9.9984264e-01, 1.2757885e-04,\n        5.4037144e-16],\n       [6.8602812e-01, 1.9168968e-01, 6.6282940e-03, 8.3260022e-02,\n        3.2393940e-02],\n       [7.8717911e-01, 8.3123576e-03, 7.1034902e-03, 1.9738004e-01,\n        2.4930268e-05],\n       [2.5488302e-01, 7.3050028e-01, 8.6609060e-03, 3.3862768e-03,\n        2.5695467e-03],\n       [4.0107757e-02, 4.5974612e-02, 3.6110908e-08, 3.2376161e-01,\n        5.9015602e-01]], dtype=float32) has invalid type <class 'numpy.ndarray'>, must be a string or Tensor. (Can not convert a ndarray into a Tensor or Operation.)"
     ]
    }
   ],
   "source": [
    "y_predict=sess.run(y_predict, \n",
    "                   feed_dict={x: x_test_batch[:10]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.2424151e-10, 7.4186384e-05, 9.9992585e-01, 4.9180056e-08,\n",
       "        4.6070851e-12],\n",
       "       [1.2405844e-04, 2.4688643e-05, 9.3271933e-07, 1.2063558e-07,\n",
       "        9.9985015e-01],\n",
       "       [1.9286692e-02, 9.6026134e-01, 1.7305428e-02, 1.8467846e-03,\n",
       "        1.2996926e-03],\n",
       "       [9.4110735e-02, 2.6780635e-01, 1.6595725e-02, 6.1970866e-01,\n",
       "        1.7785546e-03],\n",
       "       [2.2106965e-08, 1.9486179e-06, 6.0857014e-06, 1.1412323e-05,\n",
       "        9.9998057e-01]], dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 預測結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_result=sess.run(tf.argmax(y_predict,1),\n",
    "                           feed_dict={x:x_test_batch ,\n",
    "                                      y_label:y_test_batch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 4, 1, 3, 4, 2, 0, 0, 1, 4], dtype=int64)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_result[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_batch[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def show_images_labels_predict(images,labels,prediction_result):\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(8, 10)\n",
    "    for i in range(0, 10):\n",
    "        ax=plt.subplot(5,5, 1+i)\n",
    "        ax.imshow(np.reshape(images[i],(28, 28)), \n",
    "                  cmap='binary')\n",
    "        ax.set_title(\"label=\" +str(np.argmax(labels[i]))+\n",
    "                     \",predict=\"+str(prediction_result[i])\n",
    "                     ,fontsize=9) \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-c351da9073bb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mshow_images_labels_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprediction_result\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "show_images_labels_predict(data.test.images,data.test.labels,prediction_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 找出預測錯誤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mnist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-387f47ff022f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mprediction_result\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m!=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m         print(\"i=\"+str(i)+\n\u001b[0;32m      4\u001b[0m               \u001b[1;34m\"   label=\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m               \"predict=\",prediction_result[i])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mnist' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(500):\n",
    "    if prediction_result[i]!=np.argmax(mnist.test.labels[i]):\n",
    "        print(\"i=\"+str(i)+\n",
    "              \"   label=\",np.argmax(mnist.test.labels[i]),\n",
    "              \"predict=\",prediction_result[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_images_labels_predict_error(images,labels,prediction_result):\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(8, 10)\n",
    "    i=0;j=0\n",
    "    while i<10:\n",
    "        if prediction_result[j]!=np.argmax(labels[j]):\n",
    "            ax=plt.subplot(5,5, 1+i)\n",
    "            ax.imshow(np.reshape(images[j],(28, 28)), \n",
    "                      cmap='binary')\n",
    "            ax.set_title(\"j=\"+str(j)+\n",
    "                         \",l=\" +str(np.argmax(labels[j]))+\n",
    "                         \",p=\"+str(prediction_result[j])\n",
    "                         ,fontsize=9) \n",
    "            i=i+1  \n",
    "        j=j+1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeQAAADpCAYAAAD1R0STAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f07014b2a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_images_labels_predict_error(mnist.test.images,mnist.test.labels,prediction_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = saver.save(sess, \"set_model/CNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in file: saveModel/CNN_model1\n"
     ]
    }
   ],
   "source": [
    "print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged = tf.summary.merge_all()\n",
    "train_writer = tf.summary.FileWriter('log/CNN',sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sess.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
